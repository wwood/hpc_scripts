#!/usr/bin/env python3.9

# Script to more easily submit jobs to the QUT HPC queuing system

__author__ = "Ben Woodcroft, Peter Sternes"
__copyright__ = "Copyright 2022-2025"
__credits__ = ["Ben Woodcroft, Peter Sternes"]
__license__ = "GPL3"
__maintainer__ = "Ben Woodcroft, Peter Sternes"
__email__ = "benjwoodcroft near gmail.com"
__status__ = "Development"

import argparse
from argparse import RawTextHelpFormatter,SUPPRESS
import logging
import sys
import os
import tempfile
import subprocess
import getpass
import shutil
import re
from datetime import date
import json
import time
from smtplib import SMTP
import datetime
import re

DEFAULT_RAM_TO_CPU_RATIO = 1495.0 / 192.0

## Code below copied from the extern python package. Copy the code here so there are no dependencies.

def run(command, stdin=None):
    '''
    Run a subprocess.check_output() with the given command with
    'bash -c command'
    returning the stdout. If the command fails (i.e. has a non-zero exitstatus),
    raise a ExternCalledProcessError that includes the $stderr as part of
    the error message

    Parameters
    ----------
    command: str
        command to run
    stdin: str or None
        stdin to be provided to the process, to subprocess.communicate.

    Returns
    -------
    Standard output of the run command

    Exceptions
    ----------
    extern.ExternCalledProcessError including stdout and stderr of the run
    command should it return with non-zero exit status.
    '''
    #logging.debug("Running extern cmd: %s" % command)

    using_stdin = stdin is not None
    process = process = subprocess.Popen(
        ["bash",'-o','pipefail',"-c", command],
        stdin= (subprocess.PIPE if using_stdin else None),
        stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate(stdin)

    if process.returncode != 0:
        raise ExternCalledProcessError(process, command, stdout.decode(), stderr.decode())
    return stdout



#%% CLASSES ##########
######################

class ExternCalledProcessError(subprocess.CalledProcessError):
    def __init__(self, completed_process, command, stdout, stderr):
        self.command = command
        self.returncode = completed_process.returncode
        self.stderr = stderr
        self.stdout = stdout
        self.completed_process = completed_process

    def __str__(self):
        return "Command %s returned non-zero exit status %i.\n"\
            "STDERR was: %sSTDOUT was: %s" % (
                self.command,
                self.returncode,
                self.stderr,
                self.stdout)


class PbsJobInfo:
    @staticmethod
    def json(job_id):
        return json.loads(run("qstat -x -f {} -F json".format(job_id)).decode())['Jobs'][job_id]

    @staticmethod
    def status(job_id):
        json_result = PbsJobInfo.json(job_id)
        return json_result['job_state']

    @staticmethod
    def stdout_and_stderr_paths(job_id, segregated_logs_dir=None):
        json_result = PbsJobInfo.json(job_id)
        reg = re.compile("^.*?:(.*)$")
        out1 = json_result['Output_Path']
        err1 = json_result['Error_Path']
        out2, err2 = reg.match(out1).group(1), reg.match(err1).group(1)
        if segregated_logs_dir:
            return os.path.join(out2, '%s.OU' % job_id), os.path.join(err2, '%s.ER' % job_id)
        else:
            return out2, err2 

    @staticmethod
    def job_status_english(state):
        states = {}
        states['B'] = 'Array job has at least one subjob running'
        states['E'] = 'Job is exiting after having run'
        states['F'] = 'Job is finished'
        states['H'] = 'Job is held'
        states['M'] = 'Job was moved to another server'
        states['Q'] = 'Job is queued'
        states['R'] = 'Job is running'
        states['S'] = 'Job is suspended'
        states['T'] = 'Job is being moved to new location'
        states['U'] = 'Cycle-harvesting job is suspended due to keyboard activity'
        states['W'] = 'Job is waiting for its submitter-assigned start time to be reached'
        states['X'] = 'Subjob has completed execution or has been deleted'
        return states[state]


class script_format:
    @staticmethod
    def header(outfile, prelude=None, segregated_logs_dir=None):
        print('#!/bin/bash -l',file=outfile)
        print('#PBS -l ncpus={}'.format(args.cpus),file=outfile)
        print('#PBS -l ngpus={}'.format(args.gpu),file=outfile)
        if args.gpu_type:
            print('#PBS -l gpu_id={}'.format(args.gpu_type), file=outfile)
        print('#PBS -l mem={}gb'.format(mem),file=outfile)
        print('#PBS -l walltime={}:00:00'.format(hours),file=outfile)
        if args.bg and not args.no_email and not args.command_file and not (args.chunk_num or args.chunk_size):
            print('#PBS -m ae',file=outfile) # disbled emailing when running chunks. Too spammy
        print('#PBS -M {}'.format(email),file=outfile)
        if args.array:
            # normalize array format so that both "x-y" and "y" are accepted (with the latter being expanded to "1-y")
            if '-' not in args.array:
                args.array = '1-{}'.format(args.array)
            print('#PBS -J {}'.format(args.array),file=outfile)
        if args.directive:
            print('#PBS {}'.format(args.directive),file=outfile)
        if args.depend:
            depend = ":".join(args.depend)
            print('#PBS -W depend=afterok:{}'.format(depend),file=outfile)
        print('#PBS -q {}'.format(args.queue),file=outfile)
        if segregated_logs_dir:
            print('#PBS -o {}'.format(segregated_logs_dir),file=outfile)
            print('#PBS -e {}'.format(segregated_logs_dir),file=outfile)
        if args.command_file and (args.chunk_num or args.chunk_size):
            print('#PBS -N ' + command_name + str(chunkID) + '\n', file = outfile)
        else:
            print('#PBS -N {}'.format(jobname),file=outfile)
        print('. /etc/bashrc',file=outfile) # Load the bashrc file
        print("cd '{}'".format(os.getcwd()), file=outfile) # cd to current directory      
        if prelude:
            print(prelude+'\n\n',file=outfile)
        if args.scratch_data:
            for dir in args.scratch_data:
                if os.path.exists(os.path.abspath(dir)):
                    print("\n#Copy scratch-data to /scratch for processing",file=outfile)
                    print("export MSCRATCH=/scratch/cmr_mqsub/$PBS_JOBID",file=outfile)
                    print("mkdir -p $MSCRATCH",file=outfile)
                    print("cp -r -L '{}' $MSCRATCH".format(os.path.abspath(dir)),file=outfile)
                    print("CP_EXITSTATUS=$?",file=outfile)
                    print("if [[ $CP_EXITSTATUS -eq 0 ]]; then : ; else  echo 'Exit status $CP_EXITSTATUS. Exitted due to failed cp command'; exit $CP_EXITSTATUS ; fi",file=outfile)
                else:
                    raise Exception('{} not found. Exiting'.format(dir))
        if args.tmp_data:
            for dir in args.tmp_data:
                if os.path.exists(os.path.abspath(dir)):
                    print("\n#Copy tmp-data to TMPDIR for processing",file=outfile)
                    print("export MSCRATCH=$TMPDIR",file=outfile)
                    print("cp -r -L '{}' $MSCRATCH".format(os.path.abspath(dir)),file=outfile)
                    print("CP_EXITSTATUS=$?",file=outfile)
                    print("if [[ $CP_EXITSTATUS -eq 0 ]]; then : ; else  echo 'Exit status $CP_EXITSTATUS. Exitted due to failed cp command'; exit $CP_EXITSTATUS ; fi",file=outfile)
                else:
                    raise Exception('{} not found. Exiting'.format(dir))
        if args.run_tmp_dir:
            print("\n#Change to TMPDIR for processing",file=outfile)
            print("export MQSUB_TMPDIR=$TMPDIR || exit 1",file=outfile)
            print("mkdir $MQSUB_TMPDIR/output || exit 1",file=outfile)
            print("cd $MQSUB_TMPDIR/output || exit 1\n",file=outfile)
        #activate the conda environment from which this script was started
        try:
            current_conda_env = os.environ['CONDA_PREFIX']
        except:
            current_conda_env = None
        if current_conda_env:
            print("conda activate '{}'".format(current_conda_env),file=outfile)  
        if args.command_file and (args.chunk_num or args.chunk_size):
            print("\nNUM_FAILED=0\n", file=outfile)
        if args.command_file and (args.chunk_num or args.chunk_size):
            print(chunk, file=outfile)
        if args.command_file and (args.chunk_num or args.chunk_size):
            print("\necho \"Number of failed commands: $NUM_FAILED\"\n", file=outfile)
          

    @staticmethod
    def tail(outfile):
        if args.scratch_data:
            # Command to test for unexpected MSCRATCH variable changes
            # ./bin/mqsub_aqua --scratch-data bin -- export MSCRATCH=/home/aroneys/src/hpc_scripts/test_delete/asdf
            print("\n#Delete scratch-data from /scratch",file=outfile)
            print("if [[ $MSCRATCH != /scratch/cmr_mqsub/* ]]; then echo 'MSCRATCH is not in /scratch/cmr_mqsub'; exit 1; fi",file=outfile)
            print("if [[ -d $MSCRATCH ]]; then rm -rf $MSCRATCH || exit 1; fi",file=outfile)
        if args.run_tmp_dir:
            working_dir = os.getcwd()
            print("\nFINAL_EXITSTATUS=$?",file=outfile)
            print("\n#Move output from scratch",file=outfile)           
            print("cp -r $MQSUB_TMPDIR/output/* '{}' || exit 1".format(working_dir),file=outfile)
            print("exit $FINAL_EXITSTATUS",file=outfile)
            
    @staticmethod
    def report_running_host(jobinfo):
        if jobinfo['job_state'] == 'R':
            logging.info("Job is running on exec_host {}".format(jobinfo['exec_host']))

    @staticmethod
    def submit(outfile):
        if args.dry_run:
            with open(outfile.name) as script_written:
                logging.info("Script written was:\n{}".format(script_written.read()))
            logging.info("Not running qsub since this is a dry run")
            sys.exit(0)
        else:
            with open(outfile.name) as script_written:
                logging.debug("Script written was:\n{}".format(script_written.read()))

            qsub_stdout = run("TMPDIR={} qsub {}".format(args.script_tmpdir, outfile.name)).decode()
            logging.info("qsub stdout was: {}".format(qsub_stdout.rstrip()))
            if not args.bg:
                match_result = re.compile(r'^(\d+\.aqua)\n$').match(qsub_stdout)
                if match_result is None:
                    raise Exception("Unexpected output from qsub: {}".format(qsub_stdout))
                else:
                    job_id = match_result.group(1)

                last_jobinfo = PbsJobInfo.json(job_id)
                last_status = last_jobinfo['job_state']
                logging.info('First status of job is {}: {}'.format(last_status, PbsJobInfo.job_status_english(last_status)))

                script_format.report_running_host(last_jobinfo)
                while True:
                    if last_status == 'F':
                        break

                    try:
                        current_job_info = PbsJobInfo.json(job_id)
                    except:
                        print('Server issues may be occuring. Sleeping for 2 min...')
                        time.sleep(120)
                        continue

                    current_job_status = current_job_info['job_state']
                    if current_job_status != last_status:
                        logging.debug("last was '{}', current was '{}'".format(last_status, current_job_status))
                        logging.info('Now status of job is {}: {}'.format(current_job_status, PbsJobInfo.job_status_english(current_job_status)))
                        script_format.report_running_host(current_job_info)
                        last_status = current_job_status

                    if last_status == 'F':
                        break

                    logging.debug('Not finished (is {}), sleeping for {} seconds'.format(last_status, args.poll_interval))
                    time.sleep(args.poll_interval)

                logging.info("Job has finished")

                j = PbsJobInfo.json(job_id)
                exit_status = j['Exit_status'] # add some exit status info here?
                r = j['resources_used']
                logging.info("resources_used.walltime: {}".format(r['walltime']))
                logging.info("resources_used.cpupercent: {}".format(r['cpupercent']))
                logging.info("resources_used.cput: {}".format(r['cput']))
                logging.info("resources_used.vmem: {}".format(r['vmem']))

                stdout_path, stderr_path = PbsJobInfo.stdout_and_stderr_paths(job_id, segregated_logs_dir=segregated_logs_dir)
                with open(stdout_path,'r') as f: # Possible this might fail if the stdout is binary?
                    shutil.copyfileobj(f, sys.stdout)
                with open(stderr_path,'r') as f:
                    shutil.copyfileobj(f, sys.stderr)

                if not args.no_email:
                    msg = "job: {}\n".format(job_id)\
                        +"exit status: {}\n".format(exit_status)\
                        +"resources_used.walltime: {}\n".format(r['walltime'])\
                        +"resources_used.cpupercent: {}\n".format(r['cpupercent'])\
                        +"resources_used.cput: {}\n".format(r['cput'])\
                        +"resources_used.vmem: {}\n".format(r['vmem'])
                    if content_type == SCRIPT:
                        msg = msg+"\nscript_path: {}\n".format(args.script)
                    else:
                        msg = msg+"\ncommand: {}\n".format(' '.join(args.command))
                    msg = msg+"\nThis message was sent by mqsub.\n"

                    if exit_status == 0:
                        subject = 'mqsub process \'{}\' finished running with exit status 0'.format(jobname)
                    else:
                        subject = 'FAIL: mqsub process \'{}\' finished running with exit status {}' .format(jobname, exit_status)
                    with SMTP(host='localhost',port=0) as smtp:
                        smtp.sendmail('CMR_HPC',email,'Subject: {}\n\n{}'.format(subject,msg))

                if segregated_logs_dir is None: # Don't remove when we've filed the outputs away already
                    os.remove(stdout_path)
                    os.remove(stderr_path)
                sys.exit(exit_status)
            else:
                print("qsub stdout: {}".format(qsub_stdout), file=sys.stderr)

class splitter:
    @staticmethod
    def chunk_num(a, n):
        k, m = divmod(len(a), n)
        return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))

    @staticmethod
    def chunk_size(a, n):
        return (a[i:i+n] for i in range(0, len(a), n))

def setup_segregated_logs_directory(command_name):
    # Get the number of directories in the qsub_logs directory, and add 1 to it
    logs_dir1 = os.path.join(
        os.path.expanduser('~'),
        'qsub_logs',
        datetime.datetime.now().strftime("%Y-%m-%d"))
    if os.path.exists(logs_dir1):
        num_logs_dirs = len(os.listdir(logs_dir1))
    else:
        num_logs_dirs = 0
    segregated_logs_dir = os.path.join(logs_dir1, command_name+'-'+str(num_logs_dirs + 1))
    logging.info("Creating segregated log directory {}".format(segregated_logs_dir))
    # Add exist_ok due to race condition with parallel mqsubs trying to create the same folder
    os.makedirs(segregated_logs_dir, exist_ok=True)
    
    return segregated_logs_dir



#%% PARSE ###########
####################

if __name__ == '__main__':
    parser = argparse.ArgumentParser(usage=SUPPRESS,description=r'''
                            _     
                           | |    
  _ __ ___   __ _ ___ _   _| |__  
 | '_ ` _ \ / _` / __| | | | '_ \ 
 | | | | | | (_| \__ \ |_| | |_) | 
 |_| |_| |_|\__, |___/\__,_|_.__/  Centre for Microbiome Research, QUT
               | |                
               |_|       
Example usage:
    mqsub -t 24 -m 250 --hours 48 -- aviary recover --pe-1 $R1 --pe-2 $R2 --max-threads 24 --n-cores 24 --output $runID.aviary.output 
    mqsub -t 8 -m 32 --hours 48 --command-file file.txt --chunk-num 5''',formatter_class=RawTextHelpFormatter)    
    parser.add_argument('--debug', help='output debug information', action="store_true")
    #parser.add_argument('--version', help='output version information and quit',  action='version', version=repeatm.__version__)
    parser.add_argument('--quiet', help='only output errors', action="store_true")
    parser.add_argument('-t','--cpus',default=1,type=int, help="Number of CPUs to queue job with [default: 1]")
    gpu_group = parser.add_mutually_exclusive_group(required=False)
    gpu_group.add_argument('-g','--gpu',type=int,default=0, help="Number of GPUs to use [default: 0]")
    gpu_group.add_argument('--A100', action='store_true', help="Request 1 A100 GPU (old ones from lyra)")
    gpu_group.add_argument('--H100', action='store_true', help="Request 1 H100 GPU (new with aqua)")
    ram_ratio = round(DEFAULT_RAM_TO_CPU_RATIO, 2)
    parser.add_argument('-m','--mem','--ram',type=int, help=f"GB of RAM to ask for [default: num_cpus*{ram_ratio} rounded down to the nearest GB]")
    parser.add_argument('--array', help="Submit as an array job with the given number of tasks [default: Not used]")
    parser.add_argument('--directive', help="Arbitrary PBS directory to add e.g. '-l ngpus=1' to ask for a GPU [default: Not used]")
    parser.add_argument('-q','--queue', default='aqua', help="Name of queue to send to [default: aqua]")
    walltime_group = parser.add_mutually_exclusive_group()
    walltime_group.add_argument('--hours',default=48,type=int, help="Hours to run for [default: 48 hours]")
    walltime_group.add_argument('--days',type=int,help="Days to run for [default: 2]")
    walltime_group.add_argument('--weeks',type=int,help="Weeks to run for [default unspecified]")
    parser.add_argument('--name', help="Name of the job [default: first word of command]")
    parser.add_argument('--dry-run',action='store_true', help="Print script to STDOUT and do not lodge it with qsub")
    parser.add_argument('--bg',action='store_true', help="Submit the job, then quit [default: wait until job is finished before exiting]")
    parser.add_argument('--no-email',action='store_true', help="Do not send any emails, either on job finishing or aborting")
    parser.add_argument('--script', help='Script to run, or "-" for STDIN')
    parser.add_argument('--script-shell', help='Run script specified in --script with this shell [default: /bin/bash]', default='/bin/bash')
    parser.add_argument('--script-tmpdir', help="When '--script -' is specified, write the script to this location as a temporary file", default='/work/microbiome/scratch/tmp')
    parser.add_argument('--poll-interval', help="Poll the PBS server once every this many seconds [default: 30]", type=int, default=30)
    parser.add_argument('--no-executable-check', help="Usually mqsub checks the executable is currently available. Don't do this [default: do check]",action='store_true')
    parser.add_argument('--command-file',dest='command_file', help="A file with list of newline separated commands to be split into chunks and submitted. One command per line. mqsub --command-file <file.txt> --chunk-num <int>")
    parser.add_argument('--chunk-num',type=int,dest='chunk_num', help='Number of chunks to divide the commands (from --command-file) into')
    parser.add_argument('--chunk-size',type=int,dest='chunk_size', help='Number of commands (from --command-file) per a chunk ')
    parser.add_argument('--prelude', help='Code from this file will be run before each chunk')
    temp_data_group = parser.add_mutually_exclusive_group()
    temp_data_group.add_argument('--scratch-data', dest='scratch_data', nargs='+', help='Data to be copied to a scratch space prior to running the main command(s). Useful for databases used in large chunks of jobs. Use $MSCRATCH to refer to the location.')
    temp_data_group.add_argument('--tmp-data', dest='tmp_data', nargs='+', help='Data to be copied to a tmp space prior to running the main command(s). Useful for databases used in large chunks of jobs. Use $TMPDIR to refer to the location. tmp space can fill up if you are running many in parallel, in which case use --scratch-data instead.')
    parser.add_argument('--run-tmp-dir', dest='run_tmp_dir',action='store_true', help='Executes your command(s) on the local SSD ($TMPDIR/mqsub_processing) of a node. IMPORTANT: Use absolute paths for your input files, and a relative path for your output.')
    parser.add_argument('--depend', nargs='+', help='Space separated list of ids for jobs this job should depend on.')
    parser.add_argument('--segregated-log-files', action='store_true', help='Put log files in ~/qsub_logs/<date>/<directory> instead of the current working directory.')
    parser.add_argument('command',nargs='*',help='command to be run')
    parser.epilog = '''
----------------------------------------------------------------------------------------------------------
Full README can be found on the CMR github - https://github.com/centre-for-microbiome-research/hpc_scripts
Further information can also be found in the CMR Compute Notes -  https://tinyurl.com/cmr-internal-compute
 
'''
    args = parser.parse_args()

    # Setup logging
    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    SCRIPT = 'script'
    COMMAND = 'command'

    if args.script:
        content_type = SCRIPT
        if len(args.command) != 0:
            raise Exception("Cannot specify both --script and a command")
    elif len(args.command) > 0:
        content_type = COMMAND
    elif (args.chunk_num or args.chunk_size) and args.command_file is None:
        raise Exception("--chunk-num or --chunk-size specified, please specify --command-file")
    elif (args.chunk_num is None and args.chunk_size is None) and args.command_file is not None:
        raise Exception("--command-file specified, please specify --chunk-num or --chunk-size")
    elif (args.chunk_num or args.chunk_size) and args.command_file:
        args.bg = True
    else:
        raise Exception("Must specify either --script-stdin, command, or a --command-file to chunk")

    def strip_ansi_codes(s):
        return re.sub(r'\x1b\[[0-9;]*m', '', s)

    if os.path.exists('/usr/local/bin/time_until_outage.sh'):
        raw_time_left = run('bash /usr/local/bin/time_until_outage.sh').decode().rstrip().split(':')[0]
        time_left = strip_ansi_codes(raw_time_left)
    else:
        time_left = int(7890000) 
    if args.hours is not None:
        time_required = int(args.hours)
    if args.days is not None:
        time_required = int(args.days) * 24
    if args.weeks is not None:
        time_required = int(args.weeks) * 168
    if time_required > 48:
        time_required = 48 # This is only for comparison with time_left - the walltime set in the PBS script is changed elsewhere.
    if time_left == "TBA":
        sys.stderr.write('\n###################################################################')
        sys.stderr.write('\nWARNING: The time until server maintenance is not available. Maintenance is likely imminent.')
        sys.stderr.write('\n###################################################################\n\n')
    elif int(time_required) > int(time_left):
        sys.stderr.write('\n###################################################################')
        sys.stderr.write('\nWARNING: The requested walltime coincides with server maintenance.')
        sys.stderr.write('\n###################################################################\n\n')

    if args.mem is None:
        mem = int(DEFAULT_RAM_TO_CPU_RATIO*args.cpus)
    else:
        mem = args.mem
    if mem < 8 and args.queue == 'aqua' and (args.scratch_data or args.tmp_data or args.run_tmp_dir):
        mem = 8
        logging.debug("Using RAM {}".format(mem))

    hours = 'programming_error'
    if args.weeks is not None:
        hours = 168*args.weeks
    elif args.days is not None:
        hours = 24*args.days
    else:
        hours = args.hours
    if hours > 48:
        logging.warning("The requested walltime is greater than 48 hours. Truncating to 48 since aquarius cannot currently handle longer running jobs.")
        hours = 48       

    whoami = getpass.getuser()
    email = '{}@qut.edu.au'.format(whoami)
    logging.debug("Using email address: {}".format(email))

    if args.A100:
        args.gpu = 1
        args.gpu_type = 'A100'
    elif args.H100:
        args.gpu = 1
        args.gpu_type = 'H100'
    else:
        args.gpu_type = None


#%% RUN CHUNKS ##############################
#############################################

    if args.command_file and (args.chunk_num or args.chunk_size):
        if args.prelude:
            logging.info("Reading prelude from {}".format(args.prelude))
            with open(args.prelude) as f:
                prelude = f.read()
        with open(args.command_file) as f:
            chunkID = 1
            commands = f.read().splitlines()
            commands = [s + ' || NUM_FAILED=$((NUM_FAILED + 1))' for s in commands]

            if args.name is not None:
                command_name = args.name
            elif args.name is None:
                command_name = commands[0].split()[0]
            else:
                command_name = 'chunk'

            command_name = command_name.replace("/","_").replace(".","",1).replace("=","_")

            if args.chunk_num is not None and args.chunk_size is None:
                num_chunks = int(args.chunk_num)
                chunks = list(splitter.chunk_num(commands, num_chunks))
            elif args.chunk_num is None and args.chunk_size is not None:
                num_chunks = int(args.chunk_size)
                chunks = list(splitter.chunk_size(commands, num_chunks))
            else:
                print("Please specificy either --chunk_num or --chunk_size.")

            segregated_logs_dir = None
            if args.segregated_log_files:
                segregated_logs_dir = setup_segregated_logs_directory(command_name)

            for chunk in chunks:
                with open(command_name + str(chunkID) +'.sh', 'w') as outfile:
                    chunk = "\n".join(s for s in chunk)
                    script_format.header(outfile, prelude if args.prelude else None, segregated_logs_dir)
                    script_format.tail(outfile)
                chunkID = chunkID + 1

                if args.dry_run:
                    print(open(os.path.abspath(outfile.name)).read())
                else:
                    script_format.submit(outfile)
                
                os.remove(os.path.abspath(outfile.name))


#%% REGULAR MQSUB ##############################
################################################

    else:
        with tempfile.NamedTemporaryFile(prefix='mqsub_script',suffix='.sh',mode='w') as tf:
            cmd = args.command

            if args.name:
                jobname = args.name
            elif content_type == SCRIPT:
                if args.script == '-':
                    jobname = 'stdin_mqsub'
                else:
                    jobname = os.path.basename(args.script)
            else:
                jobname = cmd[0]

            jobname = jobname.replace("/","_").replace(".","",1)
            logging.debug("Naming job as: {}".format(jobname))

            if content_type == COMMAND:
                if args.no_executable_check:
                    logging.debug("Skipping executable check as requested")
                else:
                    exe = cmd[0]
                    environment_setting = re.compile('^[A-Z_]+=')
                    exe_index = 0

                    # Deal with e.g. PATH=extra:$PATH ...
                    if args.name:
                        jobname = args.name
                    else:
                        while environment_setting.match(exe) != None:
                            logging.info("Skipping command fragment {} as detected as being an environment setting".format(exe_index))
                            if exe_index >= len(cmd):
                                raise Exception("Failed to parse an executable from the command given")
                            exe_index += 1
                            exe = cmd[exe_index]
                            jobname = exe

                        logging.debug("Testing if executable {} is available in $PATH".format(exe))
                        if shutil.which(exe) is None:
                            raise Exception("The executable {} is not available, not continuing".format(exe))
                        logging.debug("Executable {} was available, seems all good".format(exe))
            else:
                logging.debug("Not checking for executable availability as args.command not defined")

            segregated_logs_dir = None
            if args.segregated_log_files:
                segregated_logs_dir = setup_segregated_logs_directory(jobname)

            script_format.header(
                tf,
                segregated_logs_dir=segregated_logs_dir)

            if content_type == COMMAND:
                print(' '.join(cmd),file=tf)
            elif content_type == SCRIPT:
                if args.script == '-':
                    with tempfile.NamedTemporaryFile(
                        prefix='mqsub_stdin_{}_{}'.format(getpass.getuser(), date.today().strftime("%d%m%Y")),
                        suffix='.sh',
                        dir=args.script_tmpdir,
                        delete=False,
                        mode='w') as stdin_tf:

                        script_path = stdin_tf.name
                        line_count = 0
                        for l in sys.stdin:
                            stdin_tf.write(l)
                            line_count += 1
                        logging.info("Wrote {} lines of stdin to the tempfile {}".format(line_count, script_path))
                    # Delete the script after completion so it cleans up, but keep the exitstatus of the original script as the exitstatus of the qsub
                    print('{} {} && rm {}'.format(args.script_shell, script_path, script_path),file=tf)
                else:
                    script_path = os.path.abspath(args.script)
                    print('{} {}'.format(args.script_shell, script_path),file=tf)
            
            script_format.tail(tf)            
            tf.flush()
            script_format.submit(tf)
